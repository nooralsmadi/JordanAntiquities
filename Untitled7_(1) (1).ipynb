{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXnf9gXZ4zav",
        "outputId": "5cf2c07b-84c5-4a65-c7e6-62d906f7a852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.16.0-py3-none-any.whl (16.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.8.1 (from gradio)\n",
            "  Downloading gradio_client-0.8.1-py3-none-any.whl (305 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydantic>=2.0 (from gradio)\n",
            "  Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.1.7 (from gradio)\n",
            "  Downloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.8.1->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.8.1->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.16.1 (from pydantic>=2.0->gradio)\n",
            "  Downloading pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.36.0,>=0.35.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx->gradio)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=afa356913d28d29ca85ba1c0fc50b9b9ed6eb731af8f6927aaedbb6936515a10\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, typing-extensions, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, annotated-types, aiofiles, uvicorn, starlette, pydantic-core, httpcore, pydantic, httpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.14\n",
            "    Uninstalling pydantic-1.10.14:\n",
            "      Successfully uninstalled pydantic-1.10.14\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 annotated-types-0.6.0 colorama-0.4.6 fastapi-0.109.0 ffmpy-0.3.1 gradio-4.16.0 gradio-client-0.8.1 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 orjson-3.9.12 pydantic-2.6.0 pydantic-core-2.16.1 pydub-0.25.1 python-multipart-0.0.6 ruff-0.1.15 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.35.1 tomlkit-0.12.0 typing-extensions-4.9.0 uvicorn-0.27.0.post1 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "pip install gradio --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7scVhgcrxfz",
        "outputId": "103c8e8d-3678-46b6-ac15-ccf37d29e623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tTEXgI4s_h-e"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, ReLU, Dropout, Softmax, Input\n",
        "from tensorflow.keras.applications.resnet import preprocess_input as preprocess_input_resnet\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input as preprocess_input_inception\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_input_efficientnet\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet101,VGG16,InceptionV3,EfficientNetB0\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras import models\n",
        "from keras.layers import *\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import pathlib\n",
        "from glob import glob\n",
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from keras.preprocessing.image import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nlNR5EdE_-5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8324f19f-0c45-47a7-a569-bd0f46f863c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171446536/171446536 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "def build_network_RESNET101(base_model, classes):\n",
        "    \"\"\"\n",
        "    Build a custom neural network architecture by adding fully connected layers on top of a pre-trained base model.\n",
        "\n",
        "    Parameters:\n",
        "        base_model (tf.keras.Model): Pre-trained base model without the fully connected layers.\n",
        "        classes (int): Number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Custom neural network model with the specified architecture.\n",
        "    \"\"\"\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(units=1024)(x)\n",
        "    x = ReLU()(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "    x = Dropout(rate=0.75)(x)\n",
        "    x = Dense(units=classes)(x)\n",
        "    output = Softmax()(x)\n",
        "    return output\n",
        "\n",
        "#Load the weights ResNet101\n",
        "base_model = ResNet101(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_output_RESNET101 = build_network_RESNET101(base_model,6)\n",
        "model1 = Model(base_model.input, model_output_RESNET101)\n",
        "model1.load_weights('/content/drive/MyDrive/weights models 1-28/fine_tuned_RESNET101_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e80ba0TRFeT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817dd8c6-8317-41c3-f97c-8bb7077ae4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "def build_network_VGG16(base_model, classes):\n",
        "    \"\"\"\n",
        "    build a new network from a pre-trained model\n",
        "\n",
        "    parameters:\n",
        "        -base_model: pretraind model\n",
        "        -classes: number of classes to classify\n",
        "\n",
        "    \"\"\"\n",
        "    x = Flatten()(base_model.output)\n",
        "\n",
        "    x = Dense(units=256)(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "\n",
        "    x = Dense(units=classes)(x)\n",
        "    output = Softmax()(x)\n",
        "    return output\n",
        "\n",
        "#Load the weights ResNet101\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_output_VGG16 = build_network_VGG16(base_model,6)\n",
        "model2 = Model(base_model.input, model_output_VGG16)\n",
        "model2.load_weights('/content/drive/MyDrive/weights models 1-28/fine_tuned_VGG16_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a12WCSMmFe6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc0bd1f-1890-4419-90b5-d9d264050460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "def build_network_InceptionV3(base_model, classes):\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(units=256)(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "    x = Dense(units=classes)(x)\n",
        "    output = Softmax()(x)\n",
        "    return output\n",
        "\n",
        "#Load the weights ResNet101\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_output_InceptionV3 = build_network_InceptionV3(base_model,6)\n",
        "model3 = Model(base_model.input, model_output_InceptionV3)\n",
        "model3.load_weights('/content/drive/MyDrive/weights models 1-28/fine_tuned_Inception_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "74uu7hMsFfNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c96cae-f1c6-434f-8701-940c293af0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "def build_network_EfficientNetB0 (base_model, classes):\n",
        "\n",
        "    x = Flatten()(base_model.output)\n",
        "    x = Dense(units=1024)(x)\n",
        "    x = ReLU()(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "    x = Dense(units=classes)(x)\n",
        "    output = Softmax()(x)\n",
        "    return output\n",
        "\n",
        "#Load the weights ResNet101\n",
        "base_model = EfficientNetB0  (weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_output_EfficientNetB0  = build_network_EfficientNetB0 (base_model,6)\n",
        "model4 = Model(base_model.input, model_output_EfficientNetB0 )\n",
        "model4.load_weights('/content/drive/MyDrive/weights models 1-28/fine_tuned_EffecientNET B7_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lNGznYx3Ws7T"
      },
      "outputs": [],
      "source": [
        "def description(area):\n",
        "  if  area==\"Ajloun\":\n",
        "    text=\"\"\"Ajloun In the northern region lies Ajloun, known for the impressive\n",
        "            Ajloun Castle,a 12th-century fortress built by the Ayyubids. Exploring\n",
        "            the castle provides not only historical insights but also stunning views\n",
        "            of the surrounding landscape.\"\"\"\n",
        "    return text\n",
        "  elif  area==\"Jerash\":\n",
        "    text=\"\"\"Jerash Known for exceptionally well-preserved Roman ruins, Jerash boasts\n",
        "            an impressive oval-shaped forum, a colonnaded street, and the Temple of Artemis.\n",
        "            The annual Jerash Festival attracts visitors with cultural performances, adding a\n",
        "            vibrant touch to the historical site.\"\"\"\n",
        "    return text\n",
        "  elif  area==\"Petra\":\n",
        "    text=\"\"\"Petra, often referred to as the \"Rose City,\" is a UNESCO World Heritage\n",
        "            Site renowned for its rock-cut architecture and water conduit system.\n",
        "             Iconic structures like Al-Khazneh andthe Monastery showcase the ancient\n",
        "             Nabatean civilization's architectural prowess.\"\"\"\n",
        "    return text\n",
        "  elif  area==\"UmmQais\":\n",
        "    text=\"\"\"Umm Qais Nestled in the northwest, Umm Qais offers panoramic views of the\n",
        "            Sea of Galilee and the Golan Heights. Its ancient Greco-Roman ruins, including\n",
        "            a well-preserved amphitheater, provide afascinating glimpse into the historical\n",
        "            tapestry of the region\"\"\"\n",
        "    return text\n",
        "  elif  area==\"RomanAmphitheater\":\n",
        "    text=\"\"\"Roman Theater Situated in the heart of Amman, the Roman Theater is an ancient amphitheater\n",
        "            dating back to the 2nd century.Its grandeur and historical significance make it a key\n",
        "            attraction in the capital, offering visitors a glimpse into Jordan's rich past.\"\"\"\n",
        "    return text\n",
        "  elif  area==\"WadiRum\":\n",
        "    text=\"\"\"Wadi Rum, with its surreal desert landscape, features Martian-like red sand dunes,\n",
        "            towering cliffs,and ancient petroglyphs. Visitors can embark on camel or jeep tours\n",
        "            to explore this unique terrain,gaining an appreciation for both its natural beauty\n",
        "            and historical significance.\"\"\"\n",
        "    return text\n",
        "  else:\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5ucHoza997QK"
      },
      "outputs": [],
      "source": [
        "# Define class labels for the drop-down list\n",
        "class_labels = [\"Ajloun\", \"Jerash\", \"Petra\", \"RomanAmphitheater\", \"UmmQais\", \"WadiRum\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(image,model):\n",
        "\n",
        "    # Convert the image to jpg format\n",
        "    converted_img_path = f\"{image.split('.')[0]}.jpg\"\n",
        "    print(converted_img_path)\n",
        "    image = Image.open(image)\n",
        "\n",
        "    if image.mode != 'RGB':\n",
        "         image = image.convert('RGB')\n",
        "\n",
        "\n",
        "    image.save(converted_img_path, 'JPEG')\n",
        "\n",
        "    image = Image.open(converted_img_path)\n",
        "    image = image.resize((224, 224))\n",
        "    image =tf.keras.preprocessing.image.img_to_array(image)\n",
        "\n",
        "    if model=='ResNet101':\n",
        "        image = image\n",
        "\n",
        "    elif(model=='VGG16'):\n",
        "      image = preprocess_input_vgg16(image)\n",
        "\n",
        "    elif(model=='InceptionV3'):\n",
        "      image = preprocess_input_inception(image)\n",
        "\n",
        "    elif(model=='EfficientNetB0'):\n",
        "      image = preprocess_input_efficientnet(image)\n",
        "\n",
        "    if(model !='InceptionV3'):\n",
        "      image /= 255.0\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "    return image\n",
        "\n",
        "confirm_msg='Please provide more pictures to confirm.'\n",
        "de=''\n",
        "# Function to make predictions\n",
        "def predict_class(image_input,model):\n",
        "    processed_image = preprocess_image(image_input,model)\n",
        "\n",
        "    if model=='ResNet101':\n",
        "        model=model1\n",
        "\n",
        "    elif(model=='VGG16'):\n",
        "      model=model2\n",
        "\n",
        "    elif(model=='InceptionV3'):\n",
        "      model=model3\n",
        "\n",
        "    elif(model=='EfficientNetB0'):\n",
        "      model=model4\n",
        "\n",
        "\n",
        "    # Make predictions using the model\n",
        "    predictions = model.predict(processed_image)\n",
        "\n",
        "    # Get the predicted class label\n",
        "    predicted_class_index = np.argmax(predictions)\n",
        "    predicted_class = class_labels[predicted_class_index]\n",
        "    print(predicted_class)\n",
        "    # Return the predicted class label\n",
        "\n",
        "    de=description(predicted_class)\n",
        "\n",
        "    r=np.max(predictions)\n",
        "\n",
        "    if round(r,2)<0.7 and round(r,2)>0.4:\n",
        "      return str(round(r,2))+\"I think this is \"+predicted_class+\"\\n\"+confirm_msg+\"\\nAnyway \"+de\n",
        "\n",
        "    elif round(r,2)<0.4:\n",
        "      erorr_msg=\"\"\"Sorry, there seems to be an error. It could be due to this picture not\n",
        "                   representing the archaeological site or being unclear.\n",
        "                   Could you please share another image for further clarification?\"\"\"\n",
        "      return str(round(r,2))+erorr_msg\n",
        "\n",
        "    else:\n",
        "      return \"This is \"+predicted_class+\"\\n\"+de\n",
        "\n",
        "\n",
        "#print(predict_class(\"/content/drive/MyDrive/weights models 1-28/220px-Petra_Treasury.jpg\",'ResNet101'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "BgjlZBqOmoXu",
        "outputId": "f0c9dfd6-c107-426f-ee79-f35deca43c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://c1bac1b81765df7964.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c1bac1b81765df7964.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import time\n",
        "\n",
        "def print_like_dislike(x: gr.LikeData):\n",
        "    print(x.index, x.value, x.liked)\n",
        "\n",
        "\n",
        "def bot(history):\n",
        "    response = \"**That's cool!**\"\n",
        "    history[-1][1] = \"\"\n",
        "    for character in response:\n",
        "        history[-1][1] += character\n",
        "        time.sleep(0.05)\n",
        "        yield history\n",
        "\n",
        "def add_text(history, text):\n",
        "    history = history + [(text, None)]\n",
        "    return history, gr.Textbox(value=\"\", interactive=False)\n",
        "\n",
        "filepath=''\n",
        "\n",
        "def add_file(history, file):\n",
        "    filepath=file\n",
        "    history = history + [((file.name,), None)]\n",
        "    return history\n",
        "\n",
        "\n",
        "def predict_class_chat(history,btn=filepath,model='ResNet101'):\n",
        "    response = predict_class(btn,model)\n",
        "    history[-1][1] = \"\"\n",
        "    for character in response:\n",
        "        history[-1][1] += character\n",
        "        time.sleep(0.01)\n",
        "        yield history\n",
        "\n",
        "\n",
        "with gr.Blocks(mode='blocks',title=\"tourist guide in Jordan\") as demo:\n",
        "\n",
        "    chatbot = gr.Chatbot(\n",
        "        value=[[None,'Welcome to your tourist guide! To discover tourist attractions, just upload the picture, and the guide will inform you about the location, provide information, and offer some suggestions.']],\n",
        "        elem_id=\"chatbot\",\n",
        "        label=\"tourist guide in Jordan\",\n",
        "        show_label=True,\n",
        "        bubble_full_width=False,\n",
        "        avatar_images=(None, (os.path.join(os.path.dirname('avatar'), \"/content/drive/MyDrive/weights models 1-28/avatar.png\"))),\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        drop_msg=gr.Dropdown(['ResNet101','VGG16','InceptionV3','EfficientNetB0'], label=\"Select favorite tourist guide\",show_label=True,container=True)\n",
        "        btn = gr.UploadButton(\"📁 Upload a image\", file_types=[\"image\"],label='image')\n",
        "\n",
        "\n",
        "\n",
        "    drop_msg = drop_msg.input(predict_class_chat, [chatbot, btn,drop_msg], chatbot)\n",
        "\n",
        "    file_msg = btn.upload(add_file, [chatbot,btn], [chatbot], queue=False).then(\n",
        "        predict_class_chat, [chatbot, btn], chatbot\n",
        "    )\n",
        "\n",
        "    #chatbot.attach_load_event(bot(),every=None)\n",
        "    chatbot.like(print_like_dislike, None, None)\n",
        "\n",
        "\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(debug=True)\n",
        "print(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "5r5GuKiE-IYw",
        "outputId": "4a93be4a-a2f3-4cc9-dcf5-cbd20849fd40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://1cb680bf74da86a54c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1cb680bf74da86a54c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        " \"\"\"custom_css =\n",
        ".body {\n",
        "  background: url('/content/66.jpeg');\n",
        "  background-size: cover;\n",
        "  height: 100vh;\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  justify-content: center;\n",
        "  margin: 0;\n",
        "}\n",
        "\n",
        ".container {\n",
        "  background-color: rgba(255, 255, 255, 0.5);\n",
        "  padding: 20px;\n",
        "  border-radius: 15px;\n",
        "  text-align: center;\n",
        "  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        "h1 {\n",
        "  font-size: 28px;\n",
        "  color: #2c3e50;\n",
        "}\n",
        "\n",
        "p {\n",
        "  font-size: 18px;\n",
        "  color: #34495e;\n",
        "}\n",
        "\n",
        "#input_image {\n",
        "  max-width: 100%;\n",
        "  max-height: 300px;\n",
        "  border: 2px solid #3498db;\n",
        "  border-radius: 10px;\n",
        "  margin-top: 20px;\n",
        "  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        "input[type=\"file\"] {\n",
        "  margin-top: 20px;\n",
        "  padding: 10px;\n",
        "  border: 1px solid #3498db;\n",
        "  border-radius: 5px;\n",
        "}\n",
        "\n",
        "#selected_class {\n",
        "  margin-top: 10px;\n",
        "  padding: 10px;\n",
        "  border: 1px solid #3498db;\n",
        "  border-radius: 5px;\n",
        "}\n",
        "\n",
        "button {\n",
        "  margin-top: 10px;\n",
        "  padding: 12px 24px;\n",
        "  background-color: rgba(255, 179, 0, 0.7);\n",
        "  color: white;\n",
        "  border: none;\n",
        "  border-radius: 5px;\n",
        "  cursor: pointer;\n",
        "}\n",
        "\n",
        "#prediction_result {\n",
        "  margin-top: 20px;\n",
        "  font-weight: bold;\n",
        "  color: #e74c3c;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create Gradio Interface\n",
        "iface = gr.Interface(css=custom_css,theme=gr.themes.Soft(),\n",
        "\n",
        "    fn=predict_class,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"filepath\", label=\"Upload an image\"),\n",
        "        gr.Dropdown(['ResNet101','VGG16','InceptionV3','EfficientNetB0'], label=\"Select favorite model\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    live=True,\n",
        "    title=\"Archaeological Site Classification Demo\",\n",
        "    description=\"Upload an image for the archaeological places and get predictions of the name of the archaeological landmark.\",\n",
        "\n",
        ")\n",
        "\n",
        "# Launch the Gradio Interface\n",
        "iface.launch()\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}